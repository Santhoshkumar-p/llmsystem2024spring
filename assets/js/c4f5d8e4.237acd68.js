"use strict";(self.webpackChunkllmsystem=self.webpackChunkllmsystem||[]).push([[195],{3261:(e,s,n)=>{n.r(s),n.d(s,{default:()=>m});var a=n(6010),i=(n(9960),n(2263)),t=n(7961),r=n(2503);const l={features:"features_t9lD",featureSvg:"featureSvg_GfXr"};var c=n(5893);function o(){return(0,c.jsx)("section",{className:l.features,children:(0,c.jsx)("div",{className:"container",children:(0,c.jsxs)("div",{className:"col",children:[(0,c.jsxs)("div",{className:(0,a.Z)("col"),children:[(0,c.jsx)("div",{className:"text--left"}),(0,c.jsxs)("div",{className:"text--left --md",children:[(0,c.jsx)(r.Z,{as:"h1",children:"Course Description"}),(0,c.jsx)("p",{children:" Recent progress of Artificial Intelligence has been largely driven by advances in large language models (LLMs) and other generative methods. These models are often very large (e.g. 175 billion parameters for GPT3) and requires increasingly larger data to train (e.g. 300 billion tokens for ChatGPT). Training, serving, fine-tuning, and evaluating LLMs require sophisticated engineering with modern hardware and software stacks. Developing scalable systems for large language models is critical to advance AI."}),(0,c.jsx)("p",{children:"In this course, students will learn the essential skills to design and implement LLM systems. This includes algorithms and system techniques to efficiently train LLMs with huge data, efficient embedding storage and retrieval, data efficient fine-tuning, communication efficient algorithms, efficient implementation of reinforcement learning with human feedback, acceleration on GPU and other hardware, model compression for deployment, and online maintenance. We will cover the latest advances about LLM systems in machine learning, natural language processing, and system research."})]})]}),(0,c.jsxs)("div",{className:(0,a.Z)("col"),children:[(0,c.jsx)("div",{className:"text--left"}),(0,c.jsxs)("div",{className:"text--left --md",children:[(0,c.jsxs)(r.Z,{as:"h1",children:["Instructor"," "]}),(0,c.jsx)(r.Z,{as:"h3",children:(0,c.jsx)("a",{href:"https://www.cs.cmu.edu/~leili/",children:"Lei Li"})})]})]})]})})})}const d={heroBanner:"heroBanner_qdFl",buttons:"buttons_AeoN"};function h(){const{siteConfig:e}=(0,i.Z)();return(0,c.jsx)("header",{className:(0,a.Z)("hero hero--primary",d.heroBanner),children:(0,c.jsxs)("div",{className:"container",children:[(0,c.jsx)(r.Z,{as:"h1",className:"hero__title",children:e.title}),(0,c.jsx)("p",{className:"hero__subtitle",children:e.tagline}),(0,c.jsx)("div",{className:d.buttons})]})})}function m(){const{siteConfig:e}=(0,i.Z)();return(0,c.jsxs)(t.Z,{title:"LLM System",description:"Description will go into a meta tag in <head />",children:[(0,c.jsx)(h,{}),(0,c.jsx)("main",{children:(0,c.jsx)(o,{})})]})}}}]);